{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy between Bernoulli Distributions\n",
    "\n",
    "The cross entropy between two Bernoulli distributions $p = \\text{Bernoulli}(\\alpha)$ and $q = \\text{Bernoulli}(\\sigma(\\beta))$ ($\\sigma(\\cdot)$ is the sigmoid function) is\n",
    "\n",
    "$$H(p, q) = - \\big(\\alpha \\log(\\sigma(\\beta)) + (1 - \\alpha) \\log(1 - \\sigma(\\beta))\\big)$$\n",
    "\n",
    "Because of numerical instability, $\\log(1 - \\sigma(\\beta))$ in the expression above is not obtained by first computing $\\sigma(\\beta)$ and then taking the logarithm of $1 - \\sigma(\\beta)$. It is computed based on the following relationship\n",
    "\n",
    "$$\\log(1 - \\sigma(\\beta)) = \\log\\left(1 - \\frac{1}{1 + \\exp(-\\beta)}\\right) = \\log\\left(\\frac{\\exp(-\\beta)}{1 + \\exp(-\\beta)}\\right) = -\\beta + \\log\\left(\\frac{1}{1 + \\exp(-\\beta)}\\right) = \\log(\\sigma(\\beta)) - \\beta$$\n",
    "\n",
    "Using this relationship, $H(p, q)$ can be transformed into\n",
    "\n",
    "$$H(p, q) = - \\big(\\log(\\sigma(\\beta)) + (\\alpha - 1) \\beta\\big)$$\n",
    "\n",
    "$\\log(\\sigma(\\beta))$ is computed using the `torch.nn.functional.logsigmoid` function provided by PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Conditional Probability and KL Divergence\n",
    "\n",
    "### 1. Bernoulli Distribution\n",
    "\n",
    "#### 1.1 Log Conditional Probability\n",
    "\n",
    "Let $\\sigma(\\beta)$ be the mean of the Bernoulli conditional distribution and $\\alpha$ be the intensity of the pixel. The log conditional probability is given by\n",
    "\n",
    "$$\\log\\big(p(\\alpha; \\sigma(\\beta))\\big) = \\alpha \\log(\\sigma(\\beta)) + (1 - \\alpha) \\log(1 - \\sigma(\\beta)) = \\log(\\sigma(\\beta)) + (\\alpha - 1) \\beta$$\n",
    "\n",
    "#### 1.2 KL Divergence\n",
    "\n",
    "The KL divergence between two Bernoulli distributions $p = \\text{Bernoulli}(\\alpha)$ and $q = \\text{Bernoulli}(\\sigma(\\beta))$ is\n",
    "\n",
    "$$D_{\\text{KL}}(p||q) = \\big(\\alpha \\log(\\alpha) + (1 - \\alpha) \\log(1 - \\alpha)\\big) - \\big(\\alpha \\log(\\sigma(\\beta)) + (1 - \\alpha) \\log(1 - \\sigma(\\beta))\\big) = \\text{const} - \\big(\\log(\\sigma(\\beta)) + (\\alpha - 1) \\beta\\big)$$\n",
    "\n",
    "Because $\\alpha$ is a constant hyperparameter, all terms containing only $\\alpha$ need not be included in the loss function.\n",
    "\n",
    "### 2. Gaussian Distribution\n",
    "\n",
    "#### 2.1 Log Conditional Probability\n",
    "\n",
    "Let $\\beta$ and $s$ be the mean and variance of the Gaussian conditional distribution, and $\\alpha$ be the intensity of the pixel. The log conditional probability is given by\n",
    "\n",
    "$$\\log\\big(p(\\alpha; \\beta, s)\\big) = -\\frac{1}{2}\\left(\\log(2 \\pi s) + \\frac{(\\alpha - \\beta)^2}{s}\\right) = \\text{const} - \\frac{(\\alpha - \\beta)^2}{2 s}$$\n",
    "\n",
    "Because $s$ is a constant hyperparameter, all terms containing only $s$ need not be included in the computations of posterior probabilities and the loss function.\n",
    "\n",
    "#### 2.2 KL Divergence\n",
    "\n",
    "The KL divergence between two Gaussian distributions $p = \\text{Normal}(\\alpha, s)$ and $q = \\text{Normal}(\\beta, s)$ is\n",
    "\n",
    "\\begin{equation}\n",
    "D_{\\text{KL}}(p||q) = -\\frac{1}{2} \\big(\\log(2 \\pi s) + 1\\big) + \\frac{1}{2} \\left(\\log(2 \\pi s) + \\frac{s + (\\alpha - \\beta)^2}{s}\\right) = \\frac{(\\alpha - \\beta)^2}{2 s}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
